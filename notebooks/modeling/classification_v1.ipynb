{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification, version 1, new things: \n",
    "1. Deviation Metrics: now uses 3 different deviation metrics to measure route quality (instead of single metric in generate_routes) \n",
    "2. Geographic Information: new function determine_country(): Adds country/location detection based on coordinates (lat/lng) \n",
    "3. Benchmark Analysis \n",
    "    - Driver/route benchmarking calculations before modeling \n",
    "    - Average score benchmarks per driver for performance comparison \n",
    "4. Multiple Models: Instead of just LSTM, now includes: \n",
    "    - PredictionRNN_bidirectional: Bidirectional LSTM \n",
    "    - PredictionRNN: Standard LSTM \n",
    "    - PredictionAllFeaturesCNN: Convolutional Neural Network \n",
    "    - PredictionAllFeaturesGRU: Gated Recurrent Unit \n",
    "    - MultiheadAttention: Transformer Based classification\n",
    "5. Baseline Comparison Models \n",
    "    - Random Forest Regressor for comparison \n",
    "    - Cross-model performance evaluation \n",
    "6. Enhanced Evaluation Metrics \n",
    "    - McNemar test for statistical model comparison \n",
    "    - Detailed prediction categories (TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_stops = pd.read_csv('data/uni_molde_v3.csv', sep=';')\n",
    "\n",
    "data_stops\n",
    "data_stops.loc[data_stops['stop_completed_at'].isna(), 'stop_completed_at'] = \"-1\"\n",
    "data_stops.loc[data_stops['stop_arrived_at'].isna(), 'stop_arrived_at'] = \"-1\"\n",
    "\n",
    "print(data_stops.isnull().sum())\n",
    "\n",
    "\n",
    "sorted_data_stops = data_stops.sort_values(by='stop_dispatched_at', ascending=True)\n",
    "sorted_data_stops = sorted_data_stops.reset_index(drop=True)\n",
    "# sorted_data_stops['day_of_week'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.day_name()\n",
    "# sorted_data_stops['date'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.date\n",
    "\n",
    "#clustering\n",
    "locations_df = sorted_data_stops[['current_lat', 'current_lng']]\n",
    "kmeans = KMeans(n_clusters=5000, random_state=42)\n",
    "kmeans.fit(locations_df)\n",
    "sorted_data_stops['cluster'] = kmeans.labels_ + 1\n",
    "\n",
    "sorted_data_stops['location_id_craft'] = sorted_data_stops.groupby(['current_lat', 'current_lng']).ngroup()+1\n",
    "with open('output.txt', 'w') as f:\n",
    "    print(sorted_data_stops.to_string(), file=f)\n",
    "print('number of groups', sorted_data_stops['location_id_craft'].nunique())\n",
    "\n",
    "# data_stops_day= sorted_data_stops[sorted_data_stops['day_of_week'] == \"Wednesday\"]\n",
    "grouped_df = sorted_data_stops.groupby('driver_workday_id')[['driver_id', 'location_type_id', 'address_id', 'location_id','stop_dispatched_at', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'stop_completed_at', 'cluster', 'location_id_craft', 'location_is_depot']].apply(lambda x: pd.Series({\n",
    "    'driver_id': x['driver_id'].tolist(),\n",
    "    'location_type_id': x['location_type_id'].tolist(),\n",
    "    'planned_route_location': x['location_id'].tolist(),\n",
    "    'stop_dispatched_at': x['stop_dispatched_at'].tolist(),\n",
    "    'stop_arrived_at': x['stop_arrived_at'].tolist(),\n",
    "    'stop_earliest': x['stop_earliest'].tolist(),\n",
    "    'stop_latest': x['stop_latest'].tolist(),\n",
    "    'current_lat': x['current_lat'].tolist(),\n",
    "    'current_lng': x['current_lng'].tolist(),\n",
    "    'stop_completed_at': x['stop_completed_at'].tolist(),\n",
    "    'planned_route_cluster': x['cluster'].tolist(),\n",
    "    'planned_route_craft': x['location_id_craft'].tolist(),\n",
    "    'location_is_depot': x['location_is_depot'].tolist(),\n",
    "})).reset_index()\n",
    "grouped_df\n",
    "# grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_data_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(row):\n",
    "    distances = []\n",
    "    for i in range(len(row['planned_route_craft'])-1):\n",
    "        coords_1 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lng']].values[0][0])\n",
    "        coords_2 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lng']].values[0][0])\n",
    "        distances.append(geodesic(coords_1, coords_2).miles)\n",
    "    return distances\n",
    "\n",
    "def calculate_distance_actual(row):\n",
    "    distances = []\n",
    "    for i in range(len(row['actual_route_location'])-1):\n",
    "        coords_1 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['actual_route_location'][i], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['actual_route_location'][i], ['current_lng']].values[0][0])\n",
    "        coords_2 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['actual_route_location'][i+1], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['actual_route_location'][i+1], ['current_lng']].values[0][0])\n",
    "        distances.append(geodesic(coords_1, coords_2).miles)\n",
    "    return distances\n",
    "\n",
    "# Create a new column 'distance_route' in 'final_routes'\n",
    "grouped_df['distance_route'] = grouped_df.apply(calculate_distance, axis=1)\n",
    "# total_sum_distance = grouped_df['distance_route'].sum()\n",
    "# grouped_df['total_distance'] = total_sum_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20692"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['total_distance_planned'] = grouped_df['distance_route'].apply(sum)\n",
    "routes = grouped_df[grouped_df.apply(lambda row: max(row['stop_dispatched_at']) < min(row['stop_completed_at']), axis=1)]\n",
    "routes = routes.reset_index(drop=True)\n",
    "len(routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index_routes_with_na = []\n",
    "for i in range(len(routes)):\n",
    "    row = routes.iloc[i]\n",
    "    if \"-1\" in row['stop_arrived_at']:\n",
    "        index_routes_with_na.append(i)\n",
    "print(\"The number of routes where one value is NA(arrived time)\", len(index_routes_with_na))\n",
    "print(index_routes_with_na)\n",
    "routes = routes.drop(index_routes_with_na)\n",
    "routes.reset_index(drop=True)\n",
    "#\n",
    "print(len(routes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#check if it is the same driver in the route\n",
    "routes['day_of_week'] = routes['stop_arrived_at'].apply(lambda x: [pd.to_datetime(dt).day_name() for dt in x])\n",
    "routes['date'] = routes['stop_arrived_at'].apply(lambda x: [pd.to_datetime(dt).date() for dt in x])\n",
    "routes['week_numbers'] = routes['stop_arrived_at'].apply(lambda x: [pd.to_datetime(dt).isocalendar().week for dt in x])\n",
    "\n",
    "def get_mode(x):\n",
    "    return pd.Series(x).mode().iloc[0] if not pd.Series(x).mode().empty else np.nan\n",
    "\n",
    "for i in routes['driver_id']:\n",
    "    if not all(x == i[0] for x in i):\n",
    "            print('Not the same driver in the route')\n",
    "routes['driver_id'] = routes['driver_id'].apply(lambda x : x[0])\n",
    "\n",
    "\n",
    "routes['week_number'] =  routes['stop_arrived_at'].apply(lambda x: [pd.to_datetime(dt).isocalendar().week for dt in x])\n",
    "routes['date_mode'] = routes['date'].apply(get_mode)\n",
    "routes['date_mode'] = pd.to_datetime(routes['date_mode'])\n",
    "routes['last_two_weeks_count'] = routes.apply(lambda row:\n",
    "                                      routes[(routes['driver_id'] == row['driver_id']) &\n",
    "                                         (row['date_mode'] - routes['date_mode']).dt.days.between(-14, 0)].shape[0],\n",
    "                                      axis=1)\n",
    "# routes = routes[(routes['date'] < '2023-12-21') | (routes['date'] > '2024-01-14')]\n",
    "routes['location_type_id'] = routes['location_type_id'].apply(lambda x: [0 if i == 1 else 1 for i in x])\n",
    "routes['location_is_depot'] = routes['location_is_depot'].apply(lambda x: [int(value) for value in x])\n",
    "\n",
    "routes = routes[routes['planned_route_craft'].apply(lambda x: len(x) > 2)]\n",
    "routes.drop(199, inplace=True)\n",
    "routes = routes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19647"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_actual_route(df, column):\n",
    "    res_col = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sorted_dates = sorted(row['stop_arrived_at'])\n",
    "        mapping = {}\n",
    "        for i, date in enumerate(row['stop_arrived_at']):\n",
    "            mapping[date] = row[column][i]\n",
    "        res_val = [mapping[sorted_dates[i]] for i in range(len(sorted_dates))]\n",
    "        res_col.append(res_val)\n",
    "    return res_col\n",
    "\n",
    "def create_actual_route_type(df, column):\n",
    "    res_col = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sorted_dates = sorted(row['stop_arrived_at'])\n",
    "        mapping = {}\n",
    "        for i, date in enumerate(sorted_dates):\n",
    "            mapping[date] = row['location_type_id'][i]\n",
    "        res_val = [mapping[row['stop_arrived_at'][i]] for i in range(len(row['stop_arrived_at']))]\n",
    "        res_col.append(res_val)\n",
    "    return res_col\n",
    "\n",
    "routes['actual_route_location'] = create_actual_route(routes, 'planned_route_craft')\n",
    "routes['actual_route_type'] = create_actual_route_type(routes, 'planned_route_craft')\n",
    "routes['distance_actual_route'] = routes.apply(calculate_distance_actual, axis=1)\n",
    "routes['total_distance_actual'] = routes['distance_actual_route'].apply(sum)\n",
    "routes['difference_distance'] = (routes['total_distance_actual'] - routes['total_distance_planned'])/routes['total_distance_planned']\n",
    "\n",
    "#remove all the routes with length <= 2\n",
    "routes = routes[routes['planned_route_craft'].apply(lambda x: len(x) > 2)]\n",
    "routes = routes.reset_index(drop=True)\n",
    "\n",
    "len(routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "routes = routes.reset_index(drop=True)\n",
    "more_than_one_1 = routes[routes['location_is_depot'].apply(lambda x: x.count(True) > 1)]\n",
    "len(more_than_one_1), len(routes)\n",
    "\n",
    "routes['actual_route_unique'] = create_actual_route(routes, 'planned_route_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "planned_routes = routes[['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'last_two_weeks_count', 'location_type_id', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'difference_distance', 'distance_route', 'distance_actual_route', 'location_is_depot', 'location_type_id', 'date', 'planned_route_location', 'actual_route_unique', 'stop_dispatched_at','week_number']]\n",
    "actual_routes = routes['actual_route_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19647, 19647)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planned_routes_list = planned_routes['planned_route_craft'].tolist()\n",
    "actual_routes_list = actual_routes.tolist()\n",
    "\n",
    "# filtered_rows = []\n",
    "#\n",
    "# for row in actual_routes_list:\n",
    "#     if row not in planned_routes_list:\n",
    "#         filtered_rows.append(row)\n",
    "\n",
    "#duplicated removed\n",
    "# planned_routes_list = [array for i, array in enumerate(planned_routes_list) if array not in planned_routes_list[:i]]\n",
    "# actual_routes_list = [array for i, array in enumerate(filtered_rows) if array not in filtered_rows[:i]]\n",
    "len(planned_routes_list),len(actual_routes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Edit Distance\n",
    "\n",
    "def minDistance(word1, word2) -> int:\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    # dp[i][j] := min # Of operations to convert word1[0..i) to word2[0..j)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      dp[i][0] = i\n",
    "\n",
    "    for j in range(1, n + 1):\n",
    "      dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      for j in range(1, n + 1):\n",
    "        if word1[i - 1] == word2[j - 1]:\n",
    "          dp[i][j] = dp[i - 1][j - 1]\n",
    "        else:\n",
    "          dp[i][j] = min(dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Route quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_route_quality_score(planned_ranks, actual_ranks):\n",
    "    # Calculate sum of absolute differences in ranks\n",
    "    sum_of_differences = sum(abs(actual_ranks.index(x) - planned_ranks.index(x)) for x in planned_ranks)\n",
    "\n",
    "    # Calculate max possible difference\n",
    "    max_possible_difference = sum([abs(2 * i - (len(planned_ranks) + 1)) for i in range(1, len(planned_ranks) + 1)])\n",
    "\n",
    "    # Calculate route quality score\n",
    "    route_quality_score = 1 - (sum_of_differences / max_possible_difference)\n",
    "\n",
    "    return route_quality_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For each driver-day, we consider only the planned route. For each planned route we calculate how much it deviates from the actual route (e.g., use some form of edit distance - see https://link.springer.com/article/10.1007/s10732-006-9001-3?), which is then normalized (divide by max edit distance, or number of visits or something?) to the interval [0, 1]. Then, a planned route is good if this distance (between planned and actual) is less than a certain threshold and otherwise it is bad. This becomes the label (bad/good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "completed_routes_df = pd.DataFrame(columns=['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'distance_route', 'distance_actual_route', 'last_two_weeks_count', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'location_type_id', 'location_is_depot', 'date','actual_route_unique','planned_route_location', 'stop_dispatched_at', 'week_number'])\n",
    "uncompleted_routes_df = pd.DataFrame(columns=['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'distance_route', 'distance_actual_route', 'last_two_weeks_count', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'location_type_id', 'location_is_depot', 'date', 'actual_route_unique', 'planned_route_location', 'stop_dispatched_at', 'week_number'])\n",
    "#11700795\n",
    "scores = []\n",
    "edit_distances = []\n",
    "completed_routes_list = []\n",
    "uncompleted_routes_list = []\n",
    "for i in range(len(planned_routes_list)):\n",
    "    editDistance = minDistance(planned_routes_list[i], actual_routes_list[i]) / len(planned_routes_list[i])\n",
    "    score = calculate_route_quality_score(planned_routes_list[i], actual_routes_list[i])\n",
    "    scores.append(score)\n",
    "    edit_distances.append(editDistance)\n",
    "\n",
    "    # if editDistance > 0.05 and score < 0.95 and planned_routes['difference_distance'].iloc[i] > 0.01:\n",
    "    if editDistance <= 0.3 and score >= 0.7 and planned_routes['difference_distance'].iloc[i] <= 0.3:\n",
    "    # if editDistance <= 0 and score >= 1 and planned_routes['difference_distance'].iloc[i] <= 0:\n",
    "        completed_routes_list.append(planned_routes.iloc[i].to_dict())\n",
    "    else:\n",
    "        uncompleted_routes_list.append(planned_routes.iloc[i].to_dict())\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "completed_routes_df = pd.DataFrame(completed_routes_list)\n",
    "uncompleted_routes_df = pd.DataFrame(uncompleted_routes_list)\n",
    "\n",
    "print(len(completed_routes_df), len(uncompleted_routes_df))\n",
    "uncompleted_routes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in completed_routes_df['driver_id']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in completed_routes_df['driver_id']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "completed_routes_df['driver_id_sorted'] = encoding_drivers\n",
    "# final_routes = pd.concat([final_routes, pd.get_dummies(final_routes['driver_id_sorted'], prefix='encoding_drivers')], axis=1)\n",
    "completed_routes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows on Sunday: 0\n"
     ]
    }
   ],
   "source": [
    "sunday_count = completed_routes_df[completed_routes_df[\"day_of_week\"] == \"Sunday\"].shape[0]\n",
    "print(f\"Number of rows on Sunday: {sunday_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# completed_routes_list_unique = [array for i, array in enumerate(completed_routes_list) if array not in completed_routes_list[:i]]\n",
    "# uncompleted_routes_list_unique = [array for i, array in enumerate(uncompleted_routes_list) if array not in uncompleted_routes_list[:i]]\n",
    "# len(completed_routes_list_unique), len (uncompleted_routes_list_unique)\n",
    "# len(completed_routes_df), len(uncompleted_routes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_routes = pd.DataFrame({\n",
    "    'routes': completed_routes_df['planned_route_craft'].tolist() + uncompleted_routes_df['planned_route_craft'].tolist(),\n",
    "    'actual_routes': completed_routes_df['actual_route_location'].tolist() + uncompleted_routes_df['actual_route_location'].tolist(),\n",
    "    'driver_id': completed_routes_df['driver_id'].tolist() +  uncompleted_routes_df['driver_id'].tolist(),\n",
    "    'distance_route': completed_routes_df['distance_route'].tolist() + uncompleted_routes_df['distance_route'].tolist(),\n",
    "    'distance_actual_route': completed_routes_df['distance_actual_route'].tolist() + uncompleted_routes_df['distance_actual_route'].tolist(),\n",
    "    'last_two_weeks_count': completed_routes_df['last_two_weeks_count'].tolist() + uncompleted_routes_df['last_two_weeks_count'].tolist(),\n",
    "    'current_lat': completed_routes_df['current_lat'].tolist() + uncompleted_routes_df['current_lat'].tolist(),\n",
    "    'current_lng': completed_routes_df['current_lng'].tolist() + uncompleted_routes_df['current_lng'].tolist(),\n",
    "    'day_of_week': completed_routes_df['day_of_week'].tolist() + uncompleted_routes_df['day_of_week'].tolist(),\n",
    "    'date': completed_routes_df['date'].tolist() + uncompleted_routes_df['date'].tolist(),\n",
    "    'location_is_depot': completed_routes_df['location_is_depot'].tolist() + uncompleted_routes_df['location_is_depot'].tolist(),\n",
    "    'location_type_id': completed_routes_df['location_type_id'].tolist() + uncompleted_routes_df['location_type_id'].tolist(),\n",
    "    'arriving_time': completed_routes_df['stop_arrived_at'].tolist() + uncompleted_routes_df['stop_arrived_at'].tolist(),\n",
    "    'planned_route_location': completed_routes_df['planned_route_location'].tolist() + uncompleted_routes_df['planned_route_location'].tolist(),\n",
    "    'actual_route_unique': completed_routes_df['actual_route_unique'].tolist() + uncompleted_routes_df['actual_route_unique'].tolist(),\n",
    "    'stop_earliest': completed_routes_df['stop_earliest'].tolist() + uncompleted_routes_df['stop_earliest'].tolist(),\n",
    "    'stop_latest': completed_routes_df['stop_latest'].tolist() + uncompleted_routes_df['stop_latest'].tolist(),\n",
    "    'stop_dispatched_at': completed_routes_df['stop_dispatched_at'].tolist() + uncompleted_routes_df['stop_dispatched_at'].tolist(),\n",
    "    'label': [0] * len(completed_routes_df)  + [1] * len(uncompleted_routes_df),\n",
    "    'week_number': completed_routes_df['week_number'].tolist() + uncompleted_routes_df['week_number'].tolist()\n",
    "})\n",
    "final_routes['len'] = final_routes['routes'].apply(lambda x: len(x))\n",
    "final_routes\n",
    "\n",
    "# [0] * len(artificial_planned_routes)\n",
    "# artificial_planned_routes['distance_route'].tolist()\n",
    "# artificial_planned_routes['driver_id'].tolist()\n",
    "# artificial_planned_routes['common_subsequence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def determine_country(lat, lng):\n",
    "    if 57.5 <= lat <= 71.2 and 4.0 <= lng <= 31.0:\n",
    "        return 0  # Norway\n",
    "    elif 54.5 <= lat <= 57.8 and 8.0 <= lng <= 15.0:\n",
    "        return 1  # Denmark\n",
    "    else:\n",
    "        return 1\n",
    "final_routes['country_flag'] = final_routes.apply(lambda row: determine_country(row['current_lat'][0], row['current_lng'][0]), axis=1)\n",
    "\n",
    "max([len(i) for i in final_routes['routes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10864\n"
     ]
    }
   ],
   "source": [
    "#create dictionary and encode to smaller unique numbers\n",
    "locations_dic = {}\n",
    "location_count = {}\n",
    "k = 1\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in locations_dic:\n",
    "           locations_dic[location] = k\n",
    "           k += 1\n",
    "k = 1\n",
    "print(len(locations_dic))\n",
    "\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in location_count:\n",
    "            location_count[location] = 1\n",
    "       else:\n",
    "            location_count[location] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of drivers 400\n",
      "correlation 0.29898646259411266\n",
      "correlation 36\n"
     ]
    }
   ],
   "source": [
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in final_routes['driver_id']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in final_routes['driver_id']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "final_routes['driver_id_sorted'] = encoding_drivers\n",
    "# final_routes['day_of_week_encoded'] = pd.Categorical(final_routes['day_of_week']).codes+1\n",
    "\n",
    "def encode_day_of_week(row):\n",
    "    a = np.zeros((36, 7), dtype=int)\n",
    "    route_len = row['len']\n",
    "    day_encoded = row['day_of_week_encoded']\n",
    "    for i in range(36 - route_len, len(a)):\n",
    "        a[i][day_encoded-1] = 1\n",
    "    return a\n",
    "\n",
    "def encode_day_of_week_simple(row):\n",
    "    a = np.zeros(7, dtype=int)\n",
    "    day_encoded = row['day_of_week_encoded']\n",
    "    a[day_encoded - 1] = 1\n",
    "    return a\n",
    "\n",
    "final_routes['experience_feature'] = final_routes.apply(lambda x: x['len'] * [x['last_two_weeks_count']], axis = 1)\n",
    "final_routes['len_feature'] = final_routes.apply(lambda x: x['len'] * [x['len']], axis = 1)\n",
    "final_routes['driver_id_feature'] = final_routes.apply(lambda x: x['len'] * [x['driver_id_sorted']], axis = 1)\n",
    "final_routes['country_flag_feature'] = final_routes.apply(lambda x: x['len'] * [x['country_flag']], axis = 1)\n",
    "# final_routes['day_of_week_feature'] = final_routes.apply(encode_day_of_week, axis = 1)\n",
    "# final_routes['day_of_week_encoded_ext'] = final_routes.apply(encode_day_of_week_simple, axis = 1)\n",
    "\n",
    "# encoding_routes = []\n",
    "# for row in final_routes['routes']:\n",
    "#     encoding_route = []\n",
    "#     for location in row:\n",
    "#         encoding_route.append(locations_dic[location])\n",
    "#     encoding_routes.append(encoding_route)\n",
    "# final_routes['routes'] = encoding_routes\n",
    "\n",
    "print('correlation', final_routes['len'].corr(final_routes['label']))\n",
    "print('correlation', max(final_routes['len']))\n",
    "# final_routes = pd.concat([final_routes, pd.get_dummies(final_routes['driver_id_sorted'], prefix='encoding_drivers')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_routes['first_arrival'] = final_routes['arriving_time'].apply(lambda x: x[0])\n",
    "\n",
    "# Convert to datetime and sort\n",
    "final_routes['first_arrival'] = pd.to_datetime(final_routes['first_arrival'])\n",
    "final_routes = final_routes.sort_values('first_arrival').reset_index(drop=True)\n",
    "\n",
    "# Drop the temporary column\n",
    "final_routes = final_routes.drop('first_arrival', axis=1)\n",
    "final_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def analyze_routes(df):\n",
    "    rows = []\n",
    "    #encode unique location (location_id)\n",
    "    loc_unique_dic = {}\n",
    "    p = 0\n",
    "    for row in df['planned_route_location']:\n",
    "        for location in row:\n",
    "           if location not in loc_unique_dic:\n",
    "               loc_unique_dic[location] = p\n",
    "               p += 1\n",
    "\n",
    "    #encode location with the same lat, long (craft_location)\n",
    "    address_unique_dic = {}\n",
    "    p = 0\n",
    "    for row in df['routes']:\n",
    "        for location in row:\n",
    "           if location not in address_unique_dic:\n",
    "               address_unique_dic[location] = p\n",
    "               p += 1\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        route_id = idx\n",
    "        driver_id = row['driver_id_sorted']\n",
    "        planned_real = row['planned_route_location']\n",
    "        actual_real = row['actual_route_unique']\n",
    "        arriving_times = row['arriving_time']\n",
    "        planned_distances = row['distance_route']\n",
    "        actual_distances = row['distance_actual_route']\n",
    "        country_flag = row['country_flag']\n",
    "        day_of_week = row['day_of_week']\n",
    "        date = row['date']\n",
    "        experience = row['last_two_weeks_count']\n",
    "        is_depot = row['location_is_depot']\n",
    "        is_pickup = row['location_type_id']\n",
    "        craft_id = row['routes']\n",
    "        week_number = row['week_number']\n",
    "\n",
    "        #times\n",
    "        arr_times = [pd.to_datetime(ts) for ts in arriving_times]\n",
    "        earliest_times = [pd.to_datetime(ts) for ts in row['stop_earliest']]\n",
    "        latest_times = [pd.to_datetime(ts) for ts in row['stop_latest']]\n",
    "        earliest_timestamp = min(min(arr_times), min(earliest_times), min(latest_times))\n",
    "\n",
    "        arr_durations = [(t - earliest_timestamp).total_seconds()  for t in arr_times]\n",
    "        earliest_durations = [(t - earliest_timestamp).total_seconds()  for t in earliest_times]\n",
    "        latest_durations = [(t - earliest_timestamp).total_seconds()  for t in latest_times]\n",
    "        #indexes\n",
    "        map = defaultdict(lambda: [])\n",
    "        for i in range(len(actual_real)):\n",
    "            map[actual_real[i]].append(i)\n",
    "\n",
    "        map_index = defaultdict(lambda: 0)\n",
    "\n",
    "        for (idx, stop) in enumerate(planned_real):\n",
    "            planned_index = idx\n",
    "            planned_distance = 0 if planned_index == 0 else planned_distances[planned_index - 1]\n",
    "            actual_index = map[stop][map_index[stop]]\n",
    "            map_index[stop] += 1\n",
    "            actual_distance = 0 if actual_index == 0 else actual_distances[actual_index - 1]\n",
    "\n",
    "            rows.append({\n",
    "                'Route ID': route_id,\n",
    "                'Driver ID': driver_id-1,\n",
    "                'Stop ID': loc_unique_dic[planned_real[idx]],\n",
    "                'Address ID': address_unique_dic[craft_id[idx]],\n",
    "                'Week ID': week_number[idx]-1,\n",
    "                'Country': country_flag,\n",
    "                'Day of Week': day_of_week[idx],\n",
    "                'IndexP': planned_index,\n",
    "                'IndexA': actual_index,\n",
    "                'Arrived Time': round(arr_durations[idx]/60, 3),\n",
    "                'Earliest Time': round(earliest_durations[idx]/60, 3),\n",
    "                'Latest Time': round(latest_durations[idx]/60, 3),\n",
    "                # 'Dispatch': row['stop_dispatched_at'][idx],\n",
    "                # 'Arrived Time_c': arriving_times[idx],\n",
    "                # 'Time Earliest_c': row['stop_earliest'][idx],\n",
    "                # 'Time Latest_c': row['stop_latest'][idx],\n",
    "                'DistanceP': planned_distance,\n",
    "                'DistanceA': actual_distance,\n",
    "                'Depot': is_depot[idx],\n",
    "                'Delivery': is_pickup[idx],\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "newdf = analyze_routes(final_routes)\n",
    "newdf.to_excel('routes_driver_analysis_2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_routes['score_cumulative_avg'] = final_routes.groupby(['driver_id_sorted', 'day_of_week'])['label'].transform(\n",
    "        lambda x: x.expanding().mean()\n",
    "    )\n",
    "final_routes['score_cumulative_driver'] = final_routes.groupby(['driver_id_sorted'])['label'].transform(\n",
    "        lambda x: x.expanding().mean()\n",
    "    )\n",
    "final_routes['score_avg_total'] = final_routes['label'].expanding().mean()\n",
    "\n",
    "final_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Average benchmark here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# result = final_routes.groupby('driver_id_sorted')[['driver_id_sorted', 'score']].apply(\n",
    "#     lambda x: pd.Series({'count': len(x), 'score': x['score'].tolist()})\n",
    "# ).reset_index()\n",
    "#\n",
    "# sorted_result = result.sort_values(by='count', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, log_loss, brier_score_loss\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_predictions(df, scores_dict):\n",
    "    def predict(row):\n",
    "        # Try to get the score from the main dictionary\n",
    "        label = scores_dict.get((row['driver_id_sorted'], row['day_of_week']))\n",
    "\n",
    "        # If not found, fall back to the driver-only dictionary\n",
    "        if label is None:\n",
    "            print('1')\n",
    "            return 0.5\n",
    "        else:\n",
    "            return label\n",
    "\n",
    "    return df.apply(predict, axis=1)\n",
    "\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    quadratic_loss = log_loss(y_true, y_pred_proba)\n",
    "    brier_score = brier_score_loss(y_true, y_pred_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"quadratic_loss\": quadratic_loss,\n",
    "        \"brier_score\": brier_score,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of drivers 314\n",
      "10546\n",
      "17667\n",
      "lens 13823 2188 1656\n",
      "val_metrics {'acc': 0.7079524680073126, 'precision': 0.7094240837696335, 'recall': 0.7265415549597856, 'f1': 0.7178807947019867, 'roc_auc': 0.7075177372553839, 'average_precision': 0.6552798246627187, 'quadratic_loss': 1.3110032629733819, 'brier_score': 0.1963393520342656, 'true_positive': 813, 'false_positive': 333, 'true_negative': 736, 'false_negative': 306}\n",
      "test_metrics {'acc': 0.7137681159420289, 'precision': 0.7096774193548387, 'recall': 0.7350835322195705, 'f1': 0.7221570926143025, 'roc_auc': 0.7135075362809344, 'average_precision': 0.655730155170317, 'quadratic_loss': 1.3141721524216174, 'brier_score': 0.19706852026087537, 'true_positive': 616, 'false_positive': 252, 'true_negative': 566, 'false_negative': 222}\n",
      "616 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/mpbqwzf51p38zw2w423xp1fw0000gn/T/ipykernel_83336/1217222917.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  final_routes_split = final_routes.groupby(['driver_id_sorted', 'day_of_week']).apply(split_group).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def get_predictions(df, scores_dict):\n",
    "    def predict(row):\n",
    "        # Try to get the score from the main dictionary\n",
    "        label = scores_dict.get((row['driver_id_sorted'], row['day_of_week']))\n",
    "\n",
    "        # If not found, fall back to the driver-only dictionary\n",
    "        if label is None:\n",
    "            print('1')\n",
    "            return 0.5\n",
    "        else:\n",
    "            return label\n",
    "\n",
    "    return df.apply(predict, axis=1)\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    quadratic_loss = log_loss(y_true, y_pred_proba)\n",
    "    brier_score = brier_score_loss(y_true, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"quadratic_loss\": quadratic_loss,\n",
    "        \"brier_score\": brier_score,\n",
    "        \"true_positive\": tp,\n",
    "        \"false_positive\": fp,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_negative\": fn\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def split_group(group):\n",
    "    n = len(group)\n",
    "    if n < 4:  # Ensure at least 1 sample for train and 1 each for val and test\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Calculate sizes for each split\n",
    "    train_size = max(int(n * 0.8), n - 4)  # Ensure at least 2 samples total for val and test\n",
    "    val_test_size = n - train_size\n",
    "\n",
    "    # Always give the extra sample to val when odd\n",
    "    val_size = (val_test_size + 1) // 2  # This will be larger when val_test_size is odd\n",
    "    test_size = val_test_size - val_size\n",
    "\n",
    "    return pd.concat([\n",
    "        group.iloc[:train_size].assign(split='train'),\n",
    "        group.iloc[train_size:train_size+val_size].assign(split='val'),\n",
    "        group.iloc[train_size+val_size:].assign(split='test')\n",
    "    ])\n",
    "\n",
    "def categorize_predictions(df, y_true, y_pred, y_pred_proba):\n",
    "    df = df.copy()\n",
    "    df['prediction_category'] = 'Unknown'\n",
    "    df.loc[(y_true == 1) & (y_pred == 1), 'prediction_category'] = 'True Positive'\n",
    "    df.loc[(y_true == 0) & (y_pred == 1), 'prediction_category'] = 'False Positive'\n",
    "    df.loc[(y_true == 1) & (y_pred == 0), 'prediction_category'] = 'False Negative'\n",
    "    df.loc[(y_true == 0) & (y_pred == 0), 'prediction_category'] = 'True Negative'\n",
    "    df['predictive_probability'] = y_pred_proba\n",
    "    return df\n",
    "\n",
    "def save_categorized_routes(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved categorized routes to {filename}\")\n",
    "\n",
    "\n",
    "final_routes_split = final_routes.groupby(['driver_id_sorted', 'day_of_week']).apply(split_group).reset_index(drop=True)\n",
    "# Separate into train, validation, and test sets\n",
    "\n",
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in final_routes_split['driver_id_sorted']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in final_routes_split['driver_id_sorted']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "final_routes_split['driver_id_sorted'] = encoding_drivers\n",
    "\n",
    "locations_dic = {}\n",
    "location_count = {}\n",
    "k = 1\n",
    "for row in final_routes_split['routes']:\n",
    "   for location in row:\n",
    "       if location not in locations_dic:\n",
    "           locations_dic[location] = k\n",
    "           k += 1\n",
    "k = 1\n",
    "print(len(locations_dic))\n",
    "\n",
    "for row in final_routes_split['routes']:\n",
    "   for location in row:\n",
    "       if location not in location_count:\n",
    "            location_count[location] = 1\n",
    "       else:\n",
    "            location_count[location] += 1\n",
    "encoding_routes = []\n",
    "for row in final_routes_split['routes']:\n",
    "    encoding_route = []\n",
    "    for location in row:\n",
    "        encoding_route.append(locations_dic[location])\n",
    "    encoding_routes.append(encoding_route)\n",
    "final_routes_split['routes'] = encoding_routes\n",
    "\n",
    "print(len(final_routes_split))\n",
    "train_df = final_routes_split[final_routes_split['split'] == 'train'].drop('split', axis=1)\n",
    "val_df = final_routes_split[final_routes_split['split'] == 'val'].drop('split', axis=1)\n",
    "test_df = final_routes_split[final_routes_split['split'] == 'test'].drop('split', axis=1)\n",
    "\n",
    "print('lens', len(train_df), len(val_df), len(test_df))\n",
    "combined_df = pd.concat([train_df, val_df, test_df], axis=0)\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "val_y_true = val_df['label']\n",
    "average_scores_dict = train_df.groupby(['driver_id_sorted', 'day_of_week'])['score_cumulative_avg'].last().to_dict()\n",
    "average_scores_dict_driver = train_df.groupby(['driver_id_sorted'])['score_cumulative_driver'].last().to_dict()\n",
    "total_avg = train_df['score_avg_total'].iloc[-1]\n",
    "val_y_pred_proba = get_predictions(val_df, average_scores_dict)\n",
    "val_y_pred = (val_y_pred_proba.values > 0.5).astype(int)\n",
    "val_metrics = calculate_metrics(val_y_true, val_y_pred, val_y_pred_proba)\n",
    "\n",
    "# # # Calculate metrics for test set\n",
    "test_y_true = test_df['label']\n",
    "test_y_pred_proba = get_predictions(test_df, average_scores_dict)\n",
    "test_y_pred = (test_y_pred_proba.values > 0.5).astype(int)\n",
    "test_metrics = calculate_metrics(test_y_true, test_y_pred, test_y_pred_proba)\n",
    "\n",
    "# test_y_pred_proba = pd.Series([total_avg] * len(test_y_true))\n",
    "# test_y_pred = (test_y_pred_proba > 0.5).astype(int)\n",
    "# test_metrics = calculate_metrics(test_y_true, test_y_pred, test_y_pred_proba)\n",
    "\n",
    "print('val_metrics', val_metrics)\n",
    "print('test_metrics', test_metrics)\n",
    "\n",
    "test_df_group_HA = categorize_predictions(test_df, test_y_true, test_y_pred, test_y_pred_proba)\n",
    "# save_categorized_routes(test_df_categorized, 'test_routes_categorized_HA.csv')\n",
    "test_df_group_HA[test_df_group_HA['prediction_category'] == 'True Positive']['routes']\n",
    "print(len(test_df_group_HA[test_df_group_HA['prediction_category'] == 'True Positive']['routes']), len(test_df_group_HA[test_df_group_HA['prediction_category'] == 'False Positive']['routes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# After benchmark, continue model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming final_routes dataframe is available and sorted by date\n",
    "# If not, make sure to sort it by date first\n",
    "\n",
    "# Prepare the data\n",
    "rf_df = final_routes_split[['driver_id_sorted', 'day_of_week_encoded', 'label', 'split', 'routes']]\n",
    "o_day_of = pd.get_dummies(rf_df['day_of_week_encoded'], prefix='day')\n",
    "o_driver = pd.get_dummies(rf_df['driver_id_sorted'], prefix='driver')\n",
    "rf_df = pd.concat([o_day_of, o_driver, rf_df['split'], rf_df['label'], rf_df['routes']], axis=1)\n",
    "\n",
    "\n",
    "for col in rf_df.columns:\n",
    "    if col != 'split' and col != 'label' and col != 'routes':\n",
    "        rf_df[col] = rf_df[col].astype(int)\n",
    "\n",
    "train_df = rf_df[rf_df['split'] == 'train'].drop('split', axis=1)\n",
    "val_df = rf_df[rf_df['split'] == 'val'].drop('split', axis=1)\n",
    "test_df = rf_df[rf_df['split'] == 'test'].drop('split', axis=1)\n",
    "\n",
    "# Create X_train and y_train\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Create X_val and y_val (optional, but often useful)\n",
    "X_val = val_df.drop('label', axis=1)\n",
    "y_val = val_df['label']\n",
    "\n",
    "# Create X_test and y_test (optional, but often useful)\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "#\n",
    "#\n",
    "# Train the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "X_trainwr = X_train.drop('routes', axis=1)\n",
    "model.fit(X_trainwr, y_train)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    quadratic_loss = log_loss(y_true, y_pred_proba)\n",
    "    brier_score = brier_score_loss(y_true, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"quadratic_loss\": quadratic_loss,\n",
    "        \"brier_score\": brier_score,\n",
    "        \"true_positive\": tp,\n",
    "        \"false_positive\": fp,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_negative\": fn\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_proba = model.predict(X_val.drop('routes', axis=1))\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "val_metrics = calculate_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "\n",
    "y_test_pred_proba = model.predict(X_test.drop('routes', axis=1))\n",
    "y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, y_test_pred_proba)\n",
    "\n",
    "print(val_metrics)\n",
    "print(test_metrics)\n",
    "\n",
    "test_df_group_RF = categorize_predictions(test_df, y_test, y_test_pred, y_test_pred_proba)\n",
    "test_df_group_RF\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df_group_RF[test_df_group_RF['prediction_category'] == 'True Positive']['routes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ha_tp_routes = test_df_group_HA[test_df_group_HA['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "rf_tp_routes = test_df_group_RF[test_df_group_RF['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "\n",
    "# Convert lists to sets of tuples for comparison\n",
    "ha_tp_routes_set = set(tuple(route) for route in ha_tp_routes)\n",
    "rf_tp_routes_set = set(tuple(route) for route in rf_tp_routes)\n",
    "\n",
    "# Find non-overlapping routes\n",
    "ha_only_routes = ha_tp_routes_set - rf_tp_routes_set\n",
    "rf_only_routes = rf_tp_routes_set - ha_tp_routes_set\n",
    "\n",
    "# Convert back to lists for display\n",
    "ha_only_routes = [list(route) for route in ha_only_routes]\n",
    "rf_only_routes = [list(route) for route in rf_only_routes]\n",
    "\n",
    "print(\"Routes unique to HA model:\", len(ha_only_routes))\n",
    "print(\"Sample of HA-only routes:\", ha_only_routes[:5])\n",
    "\n",
    "print(\"\\nRoutes unique to RF model:\", len(rf_only_routes))\n",
    "print(\"Sample of RF-only routes:\", rf_only_routes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming final_routes dataframe and relevant imports are available\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.drop('routes', axis=1).values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val.drop('routes', axis=1).values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test.drop('routes', axis=1).values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleNN(X_train.shape[1]-1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_true = y_test_tensor\n",
    "\n",
    "y_pred = y_pred.numpy()\n",
    "y_true = y_true.numpy()\n",
    "\n",
    "y_test_pred = (y_pred > 0.5).astype(int)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, y_pred)\n",
    "\n",
    "print(test_metrics)\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_pred = y_test_pred.flatten()\n",
    "\n",
    "test_df_group_NN = categorize_predictions(X_test, y_test, y_test_pred, y_pred)\n",
    "test_df_group_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routes unique to HA model: 29\n",
      "Sample of HA-only routes: [[1272, 1752, 1864, 1786, 1788, 1564, 1851, 1570], [1162, 4385, 5696, 5696, 4360, 1162, 1162], [1, 1, 7431, 5001, 7201, 1144, 1144, 7202, 7437, 1148], [1272, 1573, 1573, 1574, 1272, 1633, 1813, 1777], [2116, 2141, 2174, 2164, 2781, 2148, 2498, 2271, 2413, 2274, 2410, 2252]]\n",
      "\n",
      "Routes unique to RF model: 81\n",
      "Sample of RF-only routes: [[1162, 5715, 7072, 7072, 4511, 5716, 5794, 5702, 7099, 7105], [1162, 3700, 3701, 6963, 3621, 3622, 3624, 3627, 3629, 3630], [1, 1386, 1399, 1400, 1401, 1382, 1380, 1381, 1407, 1388, 1405, 1389], [1, 991, 990, 741, 694, 1027, 1043, 311, 4993, 1050, 1050, 5160, 674, 6473, 281, 1005, 1], [1272, 1633, 1809, 1833]]\n"
     ]
    }
   ],
   "source": [
    "ha_tp_routes = test_df_group_HA[test_df_group_HA['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "nn_tp_routes = test_df_group_NN[test_df_group_NN['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "\n",
    "# Convert lists to sets of tuples for comparison\n",
    "ha_tp_routes_set = set(tuple(route) for route in ha_tp_routes)\n",
    "nn_tp_routes_set = set(tuple(route) for route in nn_tp_routes)\n",
    "\n",
    "# Find non-overlapping routes\n",
    "ha_only_routes = ha_tp_routes_set - nn_tp_routes_set\n",
    "nn_only_routes = nn_tp_routes_set - ha_tp_routes_set\n",
    "\n",
    "# Convert back to lists for display\n",
    "ha_only_routes = [list(route) for route in ha_only_routes]\n",
    "nn_only_routes = [list(route) for route in nn_only_routes]\n",
    "\n",
    "print(\"Routes unique to HA model:\", len(ha_only_routes))\n",
    "print(\"Sample of HA-only routes:\", ha_only_routes[:5])\n",
    "\n",
    "print(\"\\nRoutes unique to RF model:\", len(nn_only_routes))\n",
    "print(\"Sample of RF-only routes:\", nn_only_routes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656 965\n",
      "1656 838\n"
     ]
    }
   ],
   "source": [
    "# unique_days = final_routes['day_of_week'].unique()\n",
    "# day_of_week_encoded = pd.get_dummies(final_routes['day_of_week'], prefix='day_of_week')\n",
    "# final_routes = pd.concat([final_routes, day_of_week_encoded], axis=1)\n",
    "# final_routes\n",
    "print(len(y_pred), len(y_pred[y_pred>0.5]))\n",
    "print(len(y_test), len(y_test[y_test>0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def padding_(routes, route_len):\n",
    "    features = np.zeros((len(routes), route_len),dtype=np.float16)\n",
    "    for ii, route in enumerate(routes):\n",
    "        if len(route) != 0:\n",
    "            features[ii, -len(route):] = np.array(route)[:route_len]\n",
    "    return features\n",
    "\n",
    "X = final_routes_split.drop(columns = ['driver_id', 'len', 'driver_id_sorted','day_of_week_encoded'])\n",
    "max_route_length = max(len(item) for item in final_routes_split['routes'])\n",
    "y = np.array(final_routes_split['label'])\n",
    "# X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), X.to_numpy()[:,2:]], axis=1)\n",
    "# X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), padding_(X['experience_feature'], max_route_length), padding_(X['len_feature'], max_route_length), padding_(X['driver_id_feature'], max_route_length)], axis=1)\n",
    "# X = X.astype(np.float16)\n",
    "# X = X.astype(int) #for boolean values, to converst from string to int\n",
    "# final_routes\n",
    "final_routes_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Driver id and location id counting with transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in final_routes_split['driver_id_sorted']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in final_routes_split['driver_id_sorted']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "final_routes_split['driver_id_sorted'] = encoding_drivers\n",
    "\n",
    "locations_dic = {}\n",
    "location_count = {}\n",
    "k = 1\n",
    "for row in final_routes_split['routes']:\n",
    "   for location in row:\n",
    "       if location not in locations_dic:\n",
    "           locations_dic[location] = k\n",
    "           k += 1\n",
    "k = 1\n",
    "print(len(locations_dic))\n",
    "\n",
    "for row in final_routes_split['routes']:\n",
    "   for location in row:\n",
    "       if location not in location_count:\n",
    "            location_count[location] = 1\n",
    "       else:\n",
    "            location_count[location] += 1\n",
    "encoding_routes = []\n",
    "for row in final_routes_split['routes']:\n",
    "    encoding_route = []\n",
    "    for location in row:\n",
    "        encoding_route.append(locations_dic[location])\n",
    "    encoding_routes.append(encoding_route)\n",
    "final_routes_split['routes'] = encoding_routes\n",
    "\n",
    "final_routes_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## List of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "seed_value = 42\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "g = torch.Generator()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Additional steps if using DataLoaders (to ensure reproducibility in data loading)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def reset_random():\n",
    "    g.manual_seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionRNN_bidirectional(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size, vocab_size_driv, vocab_size_len,hidden_dim,embedding_dim,embedding_dim_driv, output_dim,additional_feature_count,drop_prob=0.5):\n",
    "        super(PredictionRNN,self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "        self.vocab_size_len = vocab_size_len\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "        # embedding_dim_driv+2\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.no_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        # The output of the BiLSTM will have twice the hidden size due to bidirectionality\n",
    "        self.lstm_output_size = self.hidden_dim * 2\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "        # self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        # self.fc_static_1 = nn.Linear(embedding_dim_driv+embedding_dim_len+9, 16)\n",
    "        self.fc_static_1 = nn.Linear(embedding_dim_driv+9, 128)\n",
    "        # self.fc_static_2 = nn.Linear(128, 128)\n",
    "        # # self.fc = nn.Linear(36, output_dim)\n",
    "        # # self.fc = nn.Linear(1316, output_dim)\n",
    "        self.fc = nn.Linear(128+self.lstm_output_size,128)\n",
    "        # # self.fc = nn.Linear(128,128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x, is_training=True):\n",
    "        batch_size = len(x['routes'])\n",
    "        route_ids = x['routes'].int()\n",
    "\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        # get driver ids example\n",
    "        driver_ids = x['driver_id_sorted'].int()\n",
    "        experience = x['last_two_weeks_count'].int()\n",
    "        len_feature = x['len'].int()\n",
    "        # distance = x['distance_route'].int()\n",
    "        day_of_week_feature = x['day_of_week_encoded_ext'].int()\n",
    "        country_flag = x['country_flag'].int()\n",
    "        location_is_depot = x['location_is_depot'].int()\n",
    "        location_type_id = x['location_type_id'].int()\n",
    "\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "        # ,distance.view(batch_size, max_route_length, 1)\n",
    "        # experience.view(batch_size, max_route_length, 1)\n",
    "\n",
    "        # all_embeds = torch.concatenate((embeds, distance.view(batch_size, max_route_length, 1), location_is_depot.view(batch_size, max_route_length, 1), location_type_id.view(batch_size, max_route_length, 1)), dim=2)\n",
    "        all_embeds = embeds\n",
    "\n",
    "        # all_static = torch.concatenate((embedding_driv, embedding_len, day_of_week_feature, country_flag.view(batch_size, 1),experience.view(batch_size, 1)), dim=1)\n",
    "        all_static = torch.concatenate((embedding_driv, country_flag.view(batch_size, 1),len_feature.view(batch_size, 1), day_of_week_feature), dim=1)\n",
    "        # all_static = torch.concatenate((embedding_driv, day_of_week_feature), dim=1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(all_embeds)\n",
    "        # lstm_out = lstm_out.contiguous().view(batch_size, self.lstm_output_size, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "\n",
    "        out_static = self.relu(self.fc_static_1(all_static))\n",
    "        # out_static = self.dropout(self.fc_static_2(out_static))\n",
    "        # out_static = out_static.unsqueeze(1)\n",
    "        static_expanded = out_static.unsqueeze(1).expand(-1, lstm_out.size(1), -1)\n",
    "\n",
    "        combined = self.fc(torch.cat((out, static_expanded), dim=2))\n",
    "        # out = self.fc(torch.concatenate((out, out_static), dim=1))\n",
    "        # out_static = out_static.squeeze(1).unsqueeze(1).repeat(1, 36, 1)  # New shape: [2048, 36, 64]\n",
    "        # out = self.fc(torch.cat((out, out_static), dim=2))\n",
    "        x = self.relu(combined)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        # out = self.dropout(x)\n",
    "        out = self.layer3(x)\n",
    "        # print(out.shape)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print(sig_out.shape)\n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size, vocab_size_driv, vocab_size_len,hidden_dim,embedding_dim,embedding_dim_driv, output_dim,additional_feature_count,drop_prob=0.5):\n",
    "        super(PredictionRNN,self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "        self.vocab_size_len = vocab_size_len\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        # self.fc_static_1 = nn.Linear(embedding_dim_driv+embedding_dim_len+9, 16)\n",
    "        self.fc_static_1 = nn.Linear(embedding_dim_driv+9, 128)\n",
    "        # self.fc_static_2 = nn.Linear(128, 128)\n",
    "        # # self.fc = nn.Linear(36, output_dim)\n",
    "        # # self.fc = nn.Linear(1316, output_dim)\n",
    "        self.fc = nn.Linear(64,32) #128\n",
    "        # # self.fc = nn.Linear(128,128)\n",
    "        # self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x, is_training=True):\n",
    "        batch_size = len(x['routes'])\n",
    "        route_ids = x['routes'].int()\n",
    "\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        # get driver ids example\n",
    "        driver_ids = x['driver_id_sorted'].int()\n",
    "        experience = x['last_two_weeks_count'].int()\n",
    "        len_feature = x['len'].int()\n",
    "        # distance = x['distance_route'].int()\n",
    "        day_of_week_feature = x['day_of_week_encoded_ext'].int()\n",
    "        country_flag = x['country_flag'].int()\n",
    "        location_is_depot = x['location_is_depot'].int()\n",
    "        location_type_id = x['location_type_id'].int()\n",
    "\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "        # ,distance.view(batch_size, max_route_length, 1)\n",
    "        # experience.view(batch_size, max_route_length, 1)\n",
    "\n",
    "        # all_embeds = torch.concatenate((embeds, distance.view(batch_size, max_route_length, 1), location_is_depot.view(batch_size, max_route_length, 1), location_type_id.view(batch_size, max_route_length, 1)), dim=2)\n",
    "        all_embeds = embeds\n",
    "\n",
    "        # all_static = torch.concatenate((embedding_driv, embedding_len, day_of_week_feature, country_flag.view(batch_size, 1),experience.view(batch_size, 1)), dim=1)\n",
    "        # country_flag.view(batch_size, 1),len_feature.view(batch_size, 1)\n",
    "        # len_feature.view(batch_size, 1)\n",
    "        all_static = torch.concatenate((embedding_driv, day_of_week_feature, country_flag.view(batch_size, 1), len_feature.view(batch_size, 1)), dim=1)\n",
    "        # all_static = torch.concatenate((embedding_driv, day_of_week_feature), dim=1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(all_embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, max_route_length, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "\n",
    "        out_static = self.relu(self.fc_static_1(all_static))\n",
    "        # out_static = self.dropout(self.fc_static_2(out_static))\n",
    "        # out_static = out_static.unsqueeze(1)\n",
    "        # print(out.shape, out_static.shape)\n",
    "        # out = self.fc(torch.concatenate((out, out_static), dim=1))\n",
    "        out = self.fc(out)\n",
    "        # out_static = out_static.squeeze(1).unsqueeze(1).repeat(1, 36, 1)  # New shape: [2048, 36, 64]\n",
    "        # out = self.fc(torch.cat((out, out_static), dim=2))\n",
    "        # x = self.relu(out_static)\n",
    "        # x = self.relu(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        # print(out.shape)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print(sig_out.shape)\n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionAllFeaturesCNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, vocab_size_driv, embedding_dim, embedding_dim_driv, output_dim, drop_prob=0.5):\n",
    "        super(PredictionAllFeaturesCNN, self).__init__()\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "        # embedding_dim_driv+2\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        # self.fc_static_1 = nn.Linear(embedding_dim_driv+embedding_dim_len+9, 16)\n",
    "        self.fc_static_1 = nn.Linear(embedding_dim_driv+9, 128)\n",
    "        # self.fc_static_2 = nn.Linear(128, 128)\n",
    "        # # self.fc = nn.Linear(36, output_dim)\n",
    "        # # self.fc = nn.Linear(1316, output_dim)\n",
    "        self.fc = nn.Linear(128+64,64)\n",
    "        # # self.fc = nn.Linear(128,128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        batch_size = len(x['routes'])\n",
    "        route_ids = x['routes'].int()\n",
    "\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        # get driver ids example\n",
    "        driver_ids = x['driver_id_sorted'].int()\n",
    "        experience = x['last_two_weeks_count'].int()\n",
    "        len_feature = x['len'].int()\n",
    "        # distance = x['distance_route'].int()\n",
    "        day_of_week_feature = x['day_of_week_encoded_ext'].int()\n",
    "        country_flag = x['country_flag'].int()\n",
    "        location_is_depot = x['location_is_depot'].int()\n",
    "        location_type_id = x['location_type_id'].int()\n",
    "\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "        # ,distance.view(batch_size, max_route_length, 1)\n",
    "        # experience.view(batch_size, max_route_length, 1)\n",
    "\n",
    "        # all_embeds = torch.concatenate((embeds, distance.view(batch_size, max_route_length, 1), location_is_depot.view(batch_size, max_route_length, 1), location_type_id.view(batch_size, max_route_length, 1)), dim=2)\n",
    "        all_embeds = embeds\n",
    "\n",
    "        # all_static = torch.concatenate((embedding_driv, embedding_len, day_of_week_feature, country_flag.view(batch_size, 1),experience.view(batch_size, 1)), dim=1)\n",
    "        all_static = torch.concatenate((embedding_driv, country_flag.view(batch_size, 1),len_feature.view(batch_size, 1), day_of_week_feature), dim=1)\n",
    "        # all_static = torch.concatenate((embedding_driv, day_of_week_feature), dim=1)\n",
    "\n",
    "\n",
    "        cnn_input = embeds.permute(0, 2, 1)  # Reshape for CNN: B x Feature x S\n",
    "        cnn_out = self.conv1(cnn_input)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = self.pool(cnn_out)\n",
    "        # cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        cnn_out = torch.mean(cnn_out, dim=2)\n",
    "        cnn_out = self.dropout(cnn_out)\n",
    "\n",
    "        # lstm_out, _ = self.lstm(cnn_out)\n",
    "        # # lstm_out = lstm_out.contiguous().view(batch_size, max_route_length, self.hidden_dim)\n",
    "        # lstm_out = lstm_out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        # out = self.dropout(lstm_out)\n",
    "\n",
    "        out_static = self.relu(self.fc_static_1(all_static))\n",
    "        # out_static = self.dropout(self.fc_static_2(out_static))\n",
    "        # out_static = out_static.unsqueeze(1)\n",
    "\n",
    "        combined = self.fc(torch.cat((cnn_out, out_static), dim=1))\n",
    "        # out = self.fc(torch.concatenate((out, out_static), dim=1))\n",
    "        # out_static = out_static.squeeze(1).unsqueeze(1).repeat(1, 36, 1)  # New shape: [2048, 36, 64]\n",
    "        # out = self.fc(torch.cat((out, out_static), dim=2))\n",
    "        x = self.relu(combined)\n",
    "        # x = self.relu(self.layer2(out))\n",
    "        # out = self.dropout(x)\n",
    "        out = self.layer3(x)\n",
    "        # print(out.shape)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print(sig_out.shape)\n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionAllFeaturesGRU(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, vocab_size_driv, embedding_dim, embedding_dim_driv, hidden_dim, output_dim, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "\n",
    "        # GRU layer for route features\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=no_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=drop_prob if no_layers > 1 else 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_static_1 = nn.Linear(embedding_dim_driv+8, 128)\n",
    "        self.fc = nn.Linear(128+hidden_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x['routes'])\n",
    "        route_ids = x['routes'].int()\n",
    "\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature\n",
    "\n",
    "        # Static features\n",
    "        driver_ids = x['driver_id_sorted'].int()\n",
    "        experience = x['last_two_weeks_count'].int()\n",
    "        len_feature = x['len'].int()\n",
    "        day_of_week_feature = x['day_of_week_encoded_ext'].int()\n",
    "        country_flag = x['country_flag'].int()\n",
    "        location_is_depot = x['location_is_depot'].int()\n",
    "        location_type_id = x['location_type_id'].int()\n",
    "\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "\n",
    "        all_static = torch.cat((embedding_driv, country_flag.view(batch_size, 1),\n",
    "                                len_feature.view(batch_size, 1), day_of_week_feature), dim=1)\n",
    "\n",
    "        # Process route features with GRU\n",
    "        gru_out, _ = self.gru(embeds)\n",
    "        gru_out = gru_out[:, -1, :]  # Take the last output of the sequence\n",
    "        gru_out = self.dropout(gru_out)\n",
    "\n",
    "        out_static = self.relu(self.fc_static_1(all_static))\n",
    "\n",
    "        combined = self.fc(torch.cat((gru_out, out_static), dim=1))\n",
    "        x = self.relu(combined)\n",
    "        out = self.layer3(x)\n",
    "\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
    "\n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class RunningNorm:\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim = dim\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.var = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = x\n",
    "            self.var = torch.zeros_like(x)\n",
    "        else:\n",
    "            new_mean = self.mean + (x - self.mean) / self.n\n",
    "            self.var = (self.var * (self.n - 1) + (x - self.mean) * (x - new_mean)) / self.n\n",
    "            self.mean = new_mean\n",
    "\n",
    "    def normalize(self, x):\n",
    "        if self.n == 0:\n",
    "            return x\n",
    "        return (x - self.mean) / (torch.sqrt(self.var) + 1e-8)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=35):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape should be (batch_size, sequence_length, d_model)\n",
    "        sequence_length = x.size(1)\n",
    "        x = x + self.pe[:, :sequence_length, :]\n",
    "        return x\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# \"Encoder-Only\" Style Transformer\n",
    "class NanoTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a simplified Transformer model for sequence classification.\n",
    "    It uses an embedding layer for tokens, sinusoidal positional embeddings,\n",
    "    a single Transformer block, and a final linear layer for prediction.\n",
    "\n",
    "    Args:\n",
    "      num_emb: The number of unique tokens in the vocabulary.\n",
    "      output_size: The size of the output layer (number of classes).\n",
    "      hidden_size: The dimension of the hidden layer in the Transformer block (default: 128).\n",
    "      num_heads: The number of heads in the multi-head attention layer (default: 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, vocab_size, embedding_dim_driv, vocab_size_driv, hidden_size=64, num_heads=4):\n",
    "        super(NanoTransformer, self).__init__()\n",
    "\n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim_driv = embedding_dim_driv\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        self.multihead_attn1 = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_static_1 = nn.Linear(embedding_dim_driv+9, 128)\n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "        self.fc = nn.Linear(hidden_size+128, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        bs = len(x['routes'])\n",
    "        l = len(x['routes'][0])\n",
    "        route_ids = x['routes'].int()\n",
    "        input_embs = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        driver_ids = x['driver_id_sorted'].int()\n",
    "        experience = x['last_two_weeks_count'].float()\n",
    "        len_feature = x['len'].float()\n",
    "        day_of_week_feature = x['day_of_week_encoded_ext'].int()\n",
    "        country_flag = x['country_flag'].int()\n",
    "        location_is_depot = x['location_is_depot'].int()\n",
    "        location_type_id = x['location_type_id'].int()\n",
    "\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "\n",
    "        all_static = torch.cat((embedding_driv, country_flag.view(bs, 1),\n",
    "                                len_feature.view(bs, 1), day_of_week_feature), dim=1)\n",
    "\n",
    "\n",
    "        seq_indx = torch.arange(l)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(bs, l, -1)\n",
    "        embs = input_embs + pos_emb\n",
    "        output1, attn_map1 = self.multihead_attn1(embs, embs, embs)\n",
    "        output = self.layer_norm(embs + output1)\n",
    "\n",
    "        out_static = self.relu(self.fc_static_1(all_static))\n",
    "        out_static = out_static.unsqueeze(1).expand(-1, l, -1)\n",
    "\n",
    "        out = self.fc(torch.cat((output, out_static), dim=2))\n",
    "        out = self.layer3(out)\n",
    "        sig_out = self.sig(out).squeeze(-1)\n",
    "        final_out = torch.mean(sig_out, dim=1)\n",
    "        # seq_indx = torch.arange(l)\n",
    "        # pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(bs, l, -1)\n",
    "        # embs = input_embs + pos_emb\n",
    "        # output, attn_map = self.multihead_attn(embs, embs, embs)\n",
    "        #\n",
    "        # out_static = self.bn1(self.relu(self.fc_static_1(all_static)))\n",
    "        #\n",
    "        # out_static = out_static.squeeze(1).unsqueeze(1).repeat(1, 36, 1)  # New shape: [2048, 36, 64]\n",
    "        # out = self.fc(torch.cat((output, out_static), dim=2))\n",
    "\n",
    "        # print(out.shape)\n",
    "        # out = self.layer3(out)\n",
    "        # sig_out = self.sig(out)\n",
    "        # print(sig_out.shape)\n",
    "        # sig_out = sig_out.view(bs, -1)\n",
    "        # print(sig_out.shape)\n",
    "        # sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print(sig_out.shape)\n",
    "        # raise 'ere'\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    no_layers = 2\n",
    "    vocab_size = 10547 #extra 1 for padding\n",
    "    vocab_size_driv = 317\n",
    "    vocab_size_len = 37 #extra 1 for padding\n",
    "    embedding_dim = 128 #was 64\n",
    "    embedding_dim_driv = 32 #was 64\n",
    "    # embedding_dim_len = 2 #was 64\n",
    "    output_dim = 1\n",
    "    hidden_dim = 64 #was 64\n",
    "\n",
    "    # model = PredictionAllFeaturesCNN(no_layers, vocab_size, vocab_size_driv, embedding_dim, embedding_dim_driv, output_dim, drop_prob=0.5)\n",
    "    model = PredictionRNN(no_layers,vocab_size, vocab_size_driv, vocab_size_len, hidden_dim,embedding_dim, embedding_dim_driv, output_dim,len(drivers_dic),drop_prob=0.5)\n",
    "    # model = NanoTransformer(embedding_dim, vocab_size, embedding_dim_driv, vocab_size_driv)\n",
    "    model.train()\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "# def acc(pred,label):\n",
    "#     pred = torch.round(pred.squeeze())\n",
    "#     return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "def rmse(pred, label):\n",
    "    return torch.sqrt(torch.mean((pred.squeeze() - label.squeeze())**2))\n",
    "\n",
    "# def get_precision(pred, label):\n",
    "#     pred = torch.round(pred.squeeze())\n",
    "#     true_positive = torch.sum((pred == 1) & (label.squeeze() == 1)).item()\n",
    "#     false_positive = torch.sum((pred == 1) & (label.squeeze() == 0)).item()\n",
    "#\n",
    "#     if true_positive + false_positive == 0:\n",
    "#         return 0.0, true_positive, false_positive  # Handle the case where there are no predicted positives\n",
    "#\n",
    "#     precision_value = true_positive / (true_positive + false_positive)\n",
    "#     return precision_value, true_positive, false_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RUN_NAME = 'run_5'\n",
    "def train_model(data, model, epochs = 20):\n",
    "    train_loader, valid_loader = data\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "    # train for some number of epochs\n",
    "    epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "    epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "    epoch_tr_precision,epoch_vl_precision = [],[]\n",
    "    # wandb.init(project='Route_classification', name=f'{run_name}')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        train_rmse = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            # print(labels)\n",
    "\n",
    "            model.zero_grad()\n",
    "            # print(inputs.shape, h[0].shape, h[1].shape)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            # print(output, labels)\n",
    "            # print(output.shape, labels.shape)\n",
    "            loss = criterion(output.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        val_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in valid_loader:\n",
    "                output = model(inputs, is_training=False)  # Set is_training to False for validation\n",
    "                val_loss = criterion(output.view(-1), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "\n",
    "        epoch_tr_loss.append(epoch_train_loss)\n",
    "        epoch_vl_loss.append(epoch_val_loss)\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "\n",
    "        if epoch_val_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), f'{RUN_NAME}.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "            valid_loss_min = epoch_val_loss\n",
    "        print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, log_loss, brier_score_loss, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_stats(model, data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y = []\n",
    "    all_routes = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in data:\n",
    "            y.extend(labels)\n",
    "            results = model(inputs, is_training=False).detach()  # Set is_training to False\n",
    "            y_pred.extend(results)\n",
    "            all_routes.extend(inputs['routes'].cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y)\n",
    "    y_pred_proba = np.array(y_pred)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(np.float32)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    quadratic_loss = log_loss(y_true, y_pred_proba)\n",
    "    brier_score = brier_score_loss(y_true, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"quadratic_loss\": quadratic_loss,\n",
    "        \"brier_score\": brier_score,\n",
    "        \"true_positive\": tp,\n",
    "        \"false_positive\": fp,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_negative\": fn\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_stats(model, data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y = []\n",
    "    all_inputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data:\n",
    "            y.extend(labels.cpu().numpy())\n",
    "            results = model(inputs, is_training=False).detach()\n",
    "            y_pred.extend(results.cpu().numpy())\n",
    "            all_inputs.append({k: v.cpu().numpy() for k, v in inputs.items()})\n",
    "\n",
    "    y_true = np.array(y)\n",
    "    y_pred_proba = np.array(y_pred)\n",
    "    y_pred_binary = (y_pred_proba > 0.5).astype(np.float32)\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary)\n",
    "    recall = recall_score(y_true, y_pred_binary)\n",
    "    f1 = f1_score(y_true, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    average_precision = average_precision_score(y_true, y_pred_proba)\n",
    "    quadratic_loss = log_loss(y_true, y_pred_proba)\n",
    "    brier_score = brier_score_loss(y_true, y_pred_proba)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "\n",
    "    # Combine all inputs into a single dictionary\n",
    "    combined_inputs = {k: np.concatenate([d[k] for d in all_inputs]) for k in all_inputs[0].keys()}\n",
    "\n",
    "    return {\n",
    "        \"metrics\": {\n",
    "            \"acc\": acc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"average_precision\": average_precision,\n",
    "            \"quadratic_loss\": quadratic_loss,\n",
    "            \"brier_score\": brier_score,\n",
    "            \"true_positive\": tp,\n",
    "            \"false_positive\": fp,\n",
    "            \"true_negative\": tn,\n",
    "            \"false_negative\": fn\n",
    "        },\n",
    "        \"predictions\": {\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred_proba\": y_pred_proba,\n",
    "            \"y_pred_binary\": y_pred_binary,\n",
    "            \"inputs\": combined_inputs\n",
    "        }\n",
    "    }\n",
    "\n",
    "def categorize_predictions(df, stats):\n",
    "    y_true = stats['predictions']['y_true']\n",
    "    y_pred_binary = stats['predictions']['y_pred_binary']\n",
    "    y_pred_proba = stats['predictions']['y_pred_proba']\n",
    "\n",
    "    df = df.copy()\n",
    "    df['prediction_category'] = 'Unknown'\n",
    "    df.loc[(y_true == 1) & (y_pred_binary == 1), 'prediction_category'] = 'True Positive'\n",
    "    df.loc[(y_true == 0) & (y_pred_binary == 1), 'prediction_category'] = 'False Positive'\n",
    "    df.loc[(y_true == 1) & (y_pred_binary == 0), 'prediction_category'] = 'False Negative'\n",
    "    df.loc[(y_true == 0) & (y_pred_binary == 0), 'prediction_category'] = 'True Negative'\n",
    "    df['predictive_probability'] = y_pred_proba\n",
    "\n",
    "    # Add other features from the inputs if needed\n",
    "    for feature, values in stats['predictions']['inputs'].items():\n",
    "        if feature not in df.columns:\n",
    "            df[feature] = values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df) = 13823, len(val_df) = 2188, len(test_df) = 1656\n",
      "PredictionRNN(\n",
      "  (embedding): Embedding(10547, 128)\n",
      "  (embedding_driv): Embedding(317, 32)\n",
      "  (lstm): LSTM(128, 64, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc_static_1): Linear(in_features=41, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (layer3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Epoch 1\n",
      "train_loss : 0.25870091787406374 val_loss : 0.2588515728712082\n",
      "Validation loss decreased (inf --> 0.258852).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.25046323026929584 val_loss : 0.24678972363471985\n",
      "Validation loss decreased (0.258852 --> 0.246790).  Saving model ...\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.23781342378684453 val_loss : 0.22301308065652847\n",
      "Validation loss decreased (0.246790 --> 0.223013).  Saving model ...\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.21506014040538243 val_loss : 0.1890869364142418\n",
      "Validation loss decreased (0.223013 --> 0.189087).  Saving model ...\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.1993850405727114 val_loss : 0.17696638405323029\n",
      "Validation loss decreased (0.189087 --> 0.176966).  Saving model ...\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss : 0.19468982730593 val_loss : 0.16837530210614204\n",
      "Validation loss decreased (0.176966 --> 0.168375).  Saving model ...\n",
      "==================================================\n",
      "Epoch 7\n",
      "train_loss : 0.18528507224151067 val_loss : 0.16310562565922737\n",
      "Validation loss decreased (0.168375 --> 0.163106).  Saving model ...\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss : 0.1777751467057637 val_loss : 0.15920904278755188\n",
      "Validation loss decreased (0.163106 --> 0.159209).  Saving model ...\n",
      "==================================================\n",
      "Epoch 9\n",
      "train_loss : 0.17138920511518205 val_loss : 0.15542308241128922\n",
      "Validation loss decreased (0.159209 --> 0.155423).  Saving model ...\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss : 0.1650435413633074 val_loss : 0.154672272503376\n",
      "Validation loss decreased (0.155423 --> 0.154672).  Saving model ...\n",
      "==================================================\n",
      "Epoch 11\n",
      "train_loss : 0.15963278710842133 val_loss : 0.1544497311115265\n",
      "Validation loss decreased (0.154672 --> 0.154450).  Saving model ...\n",
      "==================================================\n",
      "Epoch 12\n",
      "train_loss : 0.15499518598828996 val_loss : 0.15517009049654007\n",
      "==================================================\n",
      "Epoch 13\n",
      "train_loss : 0.1500495012317385 val_loss : 0.1556004248559475\n",
      "==================================================\n",
      "Epoch 14\n",
      "train_loss : 0.1452155719910349 val_loss : 0.15598339587450027\n",
      "==================================================\n",
      "Epoch 15\n",
      "train_loss : 0.14105187888656343 val_loss : 0.15883732959628105\n",
      "==================================================\n",
      "Epoch 16\n",
      "train_loss : 0.13645105063915253 val_loss : 0.16318939626216888\n",
      "==================================================\n",
      "Epoch 17\n",
      "train_loss : 0.1314742203269686 val_loss : 0.16418001055717468\n",
      "==================================================\n",
      "Epoch 18\n",
      "train_loss : 0.1261566121663366 val_loss : 0.16294864565134048\n",
      "==================================================\n",
      "Epoch 19\n",
      "train_loss : 0.1213853731751442 val_loss : 0.1596672721207142\n",
      "==================================================\n",
      "Epoch 20\n",
      "train_loss : 0.12215971840279442 val_loss : 0.16064263507723808\n",
      "==================================================\n",
      "Epoch 21\n",
      "train_loss : 0.12661626083510263 val_loss : 0.17929018288850784\n",
      "==================================================\n",
      "Epoch 22\n",
      "train_loss : 0.12471982198102134 val_loss : 0.1563693918287754\n",
      "==================================================\n",
      "Epoch 23\n",
      "train_loss : 0.11546480762107032 val_loss : 0.16526073217391968\n",
      "==================================================\n",
      "Epoch 24\n",
      "train_loss : 0.10499753057956696 val_loss : 0.16732091456651688\n",
      "==================================================\n",
      "Epoch 25\n",
      "train_loss : 0.10047709835427147 val_loss : 0.17578931152820587\n",
      "==================================================\n",
      "Epoch 26\n",
      "train_loss : 0.09566265451056617 val_loss : 0.17277167737483978\n",
      "==================================================\n",
      "Epoch 27\n",
      "train_loss : 0.09365406206675939 val_loss : 0.1821439191699028\n",
      "==================================================\n",
      "Epoch 28\n",
      "train_loss : 0.09061708939926964 val_loss : 0.18396461755037308\n",
      "==================================================\n",
      "Epoch 29\n",
      "train_loss : 0.08888464314596993 val_loss : 0.1751573383808136\n",
      "==================================================\n",
      "Epoch 30\n",
      "train_loss : 0.09239009342023305 val_loss : 0.18565447628498077\n",
      "==================================================\n",
      "Epoch 31\n",
      "train_loss : 0.08504309505224228 val_loss : 0.17308467626571655\n",
      "==================================================\n",
      "Epoch 32\n",
      "train_loss : 0.07953399100473948 val_loss : 0.1813272312283516\n",
      "==================================================\n",
      "Epoch 33\n",
      "train_loss : 0.07181391013520104 val_loss : 0.19368190318346024\n",
      "==================================================\n",
      "Epoch 34\n",
      "train_loss : 0.06990359351038933 val_loss : 0.18286658823490143\n",
      "==================================================\n",
      "Epoch 35\n",
      "train_loss : 0.0709063144666808 val_loss : 0.18006223440170288\n",
      "==================================================\n",
      "Epoch 36\n",
      "train_loss : 0.06562355852552823 val_loss : 0.19537672400474548\n",
      "==================================================\n",
      "Epoch 37\n",
      "train_loss : 0.06490692336644445 val_loss : 0.18473436683416367\n",
      "==================================================\n",
      "Epoch 38\n",
      "train_loss : 0.06308167214904513 val_loss : 0.19420213252305984\n",
      "==================================================\n",
      "Epoch 39\n",
      "train_loss : 0.059220075607299805 val_loss : 0.1864256039261818\n",
      "==================================================\n",
      "Epoch 40\n",
      "train_loss : 0.056707599865538735 val_loss : 0.18864666670560837\n",
      "==================================================\n",
      "PredictionRNN(\n",
      "  (embedding): Embedding(10547, 128)\n",
      "  (embedding_driv): Embedding(317, 32)\n",
      "  (lstm): LSTM(128, 64, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc_static_1): Linear(in_features=41, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (layer3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Metrics: {'acc': 0.711352657004831, 'precision': 0.711764705882353, 'recall': 0.7219570405727923, 'f1': 0.716824644549763, 'roc_auc': 0.7857630520916608, 'average_precision': 0.7993854683174876, 'quadratic_loss': 0.5660244625240187, 'brier_score': 0.1912145405549271, 'true_positive': 605, 'false_positive': 245, 'true_negative': 573, 'false_negative': 233}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "set_seed(42)\n",
    "\n",
    "FEATURE_COLUMNS = ['routes', 'driver_id_feature', 'driver_id_sorted','last_two_weeks_count', 'len', 'day_of_week_feature', 'day_of_week_encoded_ext', 'country_flag', 'location_is_depot', 'location_type_id']\n",
    "\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for col in FEATURE_COLUMNS:\n",
    "            feature = self.df[col].iloc[idx]\n",
    "            if isinstance(feature, list):\n",
    "                feature = padding_([feature], max_route_length)[0]\n",
    "            item[col] = feature\n",
    "        item['label'] = self.df['label'].iloc[idx]\n",
    "        return item, item['label']\n",
    "\n",
    "def get_data_loaders(train_df, val_df, test_df):\n",
    "    # create Tensor datasets\n",
    "    train_data = DataFrameDataset(train_df)\n",
    "    val_data = DataFrameDataset(val_df)\n",
    "    test_data = DataFrameDataset(test_df)\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 2048\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, num_workers=0, generator=g, worker_init_fn=seed_worker)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=0, generator=g, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size,num_workers=0, generator=g, worker_init_fn=seed_worker)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_df = final_routes_split[final_routes_split['split'] == 'train'].drop('split', axis=1)\n",
    "val_df = final_routes_split[final_routes_split['split'] == 'val'].drop('split', axis=1)\n",
    "test_df = final_routes_split[final_routes_split['split'] == 'test'].drop('split', axis=1)\n",
    "\n",
    "print(f'len(train_df) = {len(train_df)}, len(val_df) = {len(val_df)}, len(test_df) = {len(test_df)}')\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(train_df, val_df, test_df)\n",
    "model = get_model()\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "train_model((train_loader, val_loader), model, epochs=40)\n",
    "\n",
    "# Load the best model\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load(f'{RUN_NAME}.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_stats = get_stats(model, test_loader)\n",
    "\n",
    "print(\"Metrics:\", test_stats['metrics'])\n",
    "\n",
    "categorized_df = categorize_predictions(test_df, test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ha_tp_routes = test_df_group_HA[test_df_group_HA['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "lstm_tp_routes = categorized_df[categorized_df['prediction_category'] == 'False Positive']['routes'].tolist()\n",
    "\n",
    "print(len(categorized_df[categorized_df['prediction_category'] == 'False Negative']['routes']), len(categorized_df[categorized_df['prediction_category'] == 'False Negative']['routes']))\n",
    "# Convert lists to sets of tuples for comparison\n",
    "ha_tp_routes_set = set(tuple(route) for route in ha_tp_routes)\n",
    "lstm_tp_routes_set = set(tuple(route) for route in lstm_tp_routes)\n",
    "common_routes = ha_tp_routes_set.intersection(lstm_tp_routes_set)\n",
    "\n",
    "# Find non-overlapping routes\n",
    "ha_only_routes = ha_tp_routes_set - lstm_tp_routes_set\n",
    "lstm_only_routes = lstm_tp_routes_set - ha_tp_routes_set\n",
    "\n",
    "# Convert back to lists for display\n",
    "common_routes = [list(route) for route in common_routes]\n",
    "ha_only_routes = [list(route) for route in ha_only_routes]\n",
    "lstm_only_routes = [list(route) for route in lstm_only_routes]\n",
    "\n",
    "print(\"Routes unique to HA model:\", len(ha_only_routes), len(common_routes))\n",
    "print(\"Sample of HA-only routes:\", ha_only_routes[:5])\n",
    "\n",
    "print(\"\\nRoutes unique to LSTM model:\", len(lstm_only_routes))\n",
    "print(\"Sample of LSTM-only routes:\", lstm_only_routes[:5])\n",
    "print(len(common_routes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "CNN:\n",
    "1632: 793 label0,\n",
    "TP: 54 561 benchmark\n",
    "    75 561 model\n",
    "FP: 88 153 benchmark\n",
    "    49 153 model\n",
    "TN: 49 494 benchmark\n",
    "    86 494 model\n",
    "FN: 73 141 benchmark\n",
    "    53 141 model\n",
    "\n",
    "0.15\n",
    "TP 80 556\n",
    "   60 556\n",
    "FP: 112 149\n",
    "    36 149\n",
    "TN: 36 491\n",
    "    110 491\n",
    "FN: 60 156 benchmark\n",
    "    79 156 benchmark\n",
    "\n",
    "0.3\n",
    "TP: 45 567 benchmark\n",
    "    64 567 model\n",
    "FP: 82 167 benchmark\n",
    "    56 167 model\n",
    "TN: 55 505 benchmark\n",
    "    81 505 model\n",
    "FN: 64 155 benchmark\n",
    "    44 155 model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = 561+494, b = 54+49, c = 75+86, d = 153+141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example of calculating the mcnemar test\n",
    "from mlxtend.evaluate import mcnemar\n",
    "# define contingency table\n",
    "table = np.array([[1055, 103],\n",
    "\t\t [161, 294]])\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table)\n",
    "print(result)\n",
    "# summarize the finding\n",
    "# print('statistic=%.3f, p-value=%.10f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_stat(stats, stat_name):\n",
    "    # Calculate mean for regression stats\n",
    "    arr = np.array([item[stat_name] for item in stats])\n",
    "    return arr.mean()\n",
    "print('mse:', get_mean_stat(stats, 'mse'))\n",
    "print('rmse:', get_mean_stat(stats, 'rmse'))\n",
    "print('mae:', get_mean_stat(stats, 'mae'))\n",
    "print('r2:', get_mean_stat(stats, 'r2'))\n",
    "\n",
    "# If you want to visualize the distribution of predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stat in stats:\n",
    "    plt.scatter(stat['y'], stat['y_pred'], alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.show()\n",
    "\n",
    "# If you want to visualize the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stat in stats:\n",
    "    residuals = np.array(stat['y_pred']) - np.array(stat['y'])\n",
    "    plt.scatter(stat['y'], residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
